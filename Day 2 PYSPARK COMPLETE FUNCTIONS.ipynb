{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b7f5110-49ca-4993-95c0-29aa88884159",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#PySpark Repartition() vs Coalesce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7736e2b5-0ae8-481c-a26e-43cee15366b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "The difference between PySpark repartition() vs coalesce(), repartition() is used to increase or decrease the RDD/DataFrame partitions whereas the PySpark coalesce() is used to only decrease the number of partitions in an efficient way.\n",
    "One important point to note is, PySpark repartition() and coalesce() are very expensive operations as they shuffle the data across many partitions hence try to minimize using these as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7c13a32-55a9-43e5-afdb-04fa8ba54f04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n|movieId|               title|              genres|\n+-------+--------------------+--------------------+\n|      1|    Toy Story (1995)|Adventure|Animati...|\n|      2|      Jumanji (1995)|Adventure|Childre...|\n|      3|Grumpier Old Men ...|      Comedy|Romance|\n|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n|      5|Father of the Bri...|              Comedy|\n|      6|         Heat (1995)|Action|Crime|Thri...|\n|      7|      Sabrina (1995)|      Comedy|Romance|\n|      8| Tom and Huck (1995)|  Adventure|Children|\n|      9| Sudden Death (1995)|              Action|\n|     10|    GoldenEye (1995)|Action|Adventure|...|\n|     11|American Presiden...|Comedy|Drama|Romance|\n|     12|Dracula: Dead and...|       Comedy|Horror|\n|     13|        Balto (1995)|Adventure|Animati...|\n|     14|        Nixon (1995)|               Drama|\n|     15|Cutthroat Island ...|Action|Adventure|...|\n|     16|       Casino (1995)|         Crime|Drama|\n|     17|Sense and Sensibi...|       Drama|Romance|\n|     18|   Four Rooms (1995)|              Comedy|\n|     19|Ace Ventura: When...|              Comedy|\n|     20|  Money Train (1995)|Action|Comedy|Cri...|\n|     21|   Get Shorty (1995)|Comedy|Crime|Thri...|\n|     22|      Copycat (1995)|Crime|Drama|Horro...|\n|     23|    Assassins (1995)|Action|Crime|Thri...|\n|     24|       Powder (1995)|        Drama|Sci-Fi|\n|     25|Leaving Las Vegas...|       Drama|Romance|\n|     26|      Othello (1995)|               Drama|\n|     27| Now and Then (1995)|      Children|Drama|\n|     28|   Persuasion (1995)|       Drama|Romance|\n|     29|City of Lost Chil...|Adventure|Drama|F...|\n|     30|Shanghai Triad (Y...|         Crime|Drama|\n|     31|Dangerous Minds (...|               Drama|\n|     32|Twelve Monkeys (a...|Mystery|Sci-Fi|Th...|\n|     34|         Babe (1995)|      Children|Drama|\n|     35|   Carrington (1995)|       Drama|Romance|\n|     36|Dead Man Walking ...|         Crime|Drama|\n|     37|Across the Sea of...|    Documentary|IMAX|\n|     38| It Takes Two (1995)|     Children|Comedy|\n|     39|     Clueless (1995)|      Comedy|Romance|\n|     40|Cry, the Beloved ...|               Drama|\n|     41|  Richard III (1995)|           Drama|War|\n|     42|Dead Presidents (...|  Action|Crime|Drama|\n|     43|  Restoration (1995)|               Drama|\n|     44|Mortal Kombat (1995)|Action|Adventure|...|\n|     45|   To Die For (1995)|Comedy|Drama|Thri...|\n|     46|How to Make an Am...|       Drama|Romance|\n|     47|Seven (a.k.a. Se7...|    Mystery|Thriller|\n|     48|   Pocahontas (1995)|Animation|Childre...|\n|     49|When Night Is Fal...|       Drama|Romance|\n|     50|Usual Suspects, T...|Crime|Mystery|Thr...|\n|     52|Mighty Aphrodite ...|Comedy|Drama|Romance|\n|     53|     Lamerica (1994)|     Adventure|Drama|\n|     54|Big Green, The (1...|     Children|Comedy|\n|     55|      Georgia (1995)|               Drama|\n|     57|Home for the Holi...|               Drama|\n|     58|Postman, The (Pos...|Comedy|Drama|Romance|\n|     59|Confessional, The...|       Drama|Mystery|\n|     60|Indian in the Cup...|Adventure|Childre...|\n|     61|Eye for an Eye (1...|      Drama|Thriller|\n|     62|Mr. Holland's Opu...|               Drama|\n|     63|Don't Be a Menace...|        Comedy|Crime|\n|     64|Two if by Sea (1996)|      Comedy|Romance|\n|     65|     Bio-Dome (1996)|              Comedy|\n|     66|Lawnmower Man 2: ...|Action|Sci-Fi|Thr...|\n|     68|French Twist (Gaz...|      Comedy|Romance|\n|     69|       Friday (1995)|              Comedy|\n|     70|From Dusk Till Da...|Action|Comedy|Hor...|\n|     71|    Fair Game (1995)|              Action|\n|     72|Kicking and Screa...|        Comedy|Drama|\n|     73|Misérables, Les (...|           Drama|War|\n|     74| Bed of Roses (1996)|       Drama|Romance|\n+-------+--------------------+--------------------+\nonly showing top 70 rows\n"
     ]
    }
   ],
   "source": [
    "dfmovie=spark.read.csv(\"dbfs:/FileStore/tables/movies.csv\",header='True')\n",
    "#dfmovie.count()\n",
    "dfmovie.show(70)\n",
    "#normally 1 partition is equal to 128mb, it will varying.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c96a9d88-86dd-4750-9993-95fb194d36cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#PySpark Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a0a5bbd-f361-469a-8e03-d17776c43cff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "In PySpark RDD and DataFrame, Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, PySpark distributes broadcast variables to the workers using efficient broadcast algorithms to reduce communication costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f0f277f-122e-466d-9821-40a1a3d3e911",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Di', 'Smith', 'USA', 'California'), ('Dan', 'Rose', 'USA', 'New York'), ('Kia', 'Williams', 'USA', 'California'), ('Mary', 'Jones', 'USA', 'Florida')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('pysparkseries').getOrCreate()\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"Di\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Dan\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Kia\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Mary\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).collect()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c7e6f79-5672-4c2f-be44-8c5f08281632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Difference between DataFrame, Dataset, and RDD in Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c9e616a-8f9c-4b21-abc8-e74b23b77f49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Apache Spark provide three type of APIs\n",
    "\n",
    "RDD\n",
    "\n",
    "The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel.\n",
    "\n",
    "RDD Features:-\n",
    "Distributed collection:\n",
    "RDD uses MapReduce operations which is widely adopted for processing and generating large datasets with a parallel, distributed algorithm on a cluster. It allows users to write parallel computations, using a set of high-level operators, without having to worry about work distribution and fault tolerance.\n",
    "\n",
    "Immutable: RDDs composed of a collection of records which are partitioned. A partition is a basic unit of parallelism in an RDD, and each partition is one logical division of data which is immutable and created through some transformations on existing partitions.Immutability helps to achieve consistency in computations.\n",
    "\n",
    "Fault tolerant: In a case of we lose some partition of RDD , we can replay the transformation on that partition in lineage to achieve the same computation, rather than doing data replication across multiple nodes.This characteristic is the biggest benefit of RDD because it saves a lot of efforts in data management and replication and thus achieves faster computations.\n",
    "\n",
    "Lazy evaluations: All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset . The transformations are only computed when an action requires a result to be returned to the driver program.\n",
    "\n",
    "Functional transformations: RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset.\n",
    "\n",
    "Data processing formats:\n",
    "It can easily and efficiently process data which is structured as well as unstructured data.\n",
    "\n",
    "Programming Languages supported:\n",
    "RDD API is available in Java, Scala, Python and R.\n",
    "\n",
    "RDD Limitations:-\n",
    "No inbuilt optimization engine: When working with structured data, RDDs cannot take advantages of Spark’s advanced optimizers including catalyst optimizer and Tungsten execution engine. Developers need to optimize each RDD based on its attributes.\n",
    "\n",
    "Handling structured data: Unlike Dataframe and datasets, RDDs don’t infer the schema of the ingested data and requires the user to specify it.\n",
    "\n",
    "Dataframes\n",
    "Spark introduced Dataframes in Spark 1.3 release. Dataframe overcomes the key challenges that RDDs had.\n",
    "\n",
    "A DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a R/Python Dataframe. Along with Dataframe, Spark also introduced catalyst optimizer, which leverages advanced programming features to build an extensible query optimizer.\n",
    "\n",
    "Dataframe Features:-\n",
    "Distributed collection of Row Object: A DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database, but with richer optimizations under the hood.\n",
    "\n",
    "Data Processing: Processing structured and unstructured data formats (Avro, CSV, elastic search, and Cassandra) and storage systems (HDFS, HIVE tables, MySQL, etc). It can read and write from all these various datasources.\n",
    "\n",
    "Optimization using catalyst optimizer: It powers both SQL queries and the DataFrame API. Dataframe use catalyst tree transformation framework in four phases,\n",
    "\n",
    " 1.Analyzing a logical plan to resolve references\n",
    " 2.Logical plan optimization\n",
    " 3.Physical planning\n",
    " 4.Code generation to compile parts of the query to Java bytecode.\n",
    "Hive Compatibility: Using Spark SQL, you can run unmodified Hive queries on your existing Hive warehouses. It reuses Hive frontend and MetaStore and gives you full compatibility with existing Hive data, queries, and UDFs.\n",
    "\n",
    "Tungsten: Tungsten provides a physical execution backend whichexplicitly manages memory and dynamically generates bytecode for expression evaluation.\n",
    "\n",
    "Programming Languages supported:\n",
    "Dataframe API is available in Java, Scala, Python, and R.\n",
    "\n",
    "Dataframe Limitations:-\n",
    "Compile-time type safety: As discussed, Dataframe API does not support compile time safety which limits you from manipulating data when the structure is not know. The following example works during compile time. However, you will get a Runtime exception when executing this code.\n",
    "Example:\n",
    "\n",
    "case class Person(name : String , age : Int) \n",
    "val dataframe = sqlContext.read.json(\"people.json\") \n",
    "dataframe.filter(\"salary > 10000\").show \n",
    "=> throws Exception : cannot resolve 'salary' given input age , name\n",
    "This is challenging specially when you are working with several transformation and aggregation steps.\n",
    "\n",
    "Cannot operate on domain Object (lost domain object): Once you have transformed a domain object into dataframe, you cannot regenerate it from it. In the following example, once we have create personDF from personRDD, we won’t be recover the original RDD of Person class (RDD[Person]).\n",
    "Example:\n",
    "\n",
    "case class Person(name : String , age : Int)\n",
    "val personRDD = sc.makeRDD(Seq(Person(\"A\",10),Person(\"B\",20)))\n",
    "val personDF = sqlContext.createDataframe(personRDD)\n",
    "personDF.rdd // returns RDD[Row] , does not returns RDD[Person]\n",
    "Datasets API\n",
    "Dataset API is an extension to DataFrames that provides a type-safe, object-oriented programming interface. It is a strongly-typed, immutable collection of objects that are mapped to a relational schema.\n",
    "\n",
    "At the core of the Dataset, API is a new concept called an encoder, which is responsible for converting between JVM objects and tabular representation. The tabular representation is stored using Spark internal Tungsten binary format, allowing for operations on serialized data and improved memory utilization. Spark 1.6 comes with support for automatically generating encoders for a wide variety of types, including primitive types (e.g. String, Integer, Long), Scala case classes, and Java Beans.\n",
    "\n",
    "Dataset Features:-\n",
    "Provides best of both RDD and Dataframe: RDD(functional programming, type safe), DataFrame (relational model, Query optimazation , Tungsten execution, sorting and shuffling)\n",
    "\n",
    "Encoders: With the use of Encoders, it is easy to convert any JVM object into a Dataset, allowing users to work with both structured and unstructured data unlike Dataframe.\n",
    "\n",
    "Programming Languages supported: Datasets API is currently only available in Scala and Java. Python and R are currently not supported in version 1.6. Python support is slated for version 2.0.\n",
    "\n",
    "Type Safety: Datasets API provides compile time safety which was not available in Dataframes. In the example below, we can see how Dataset can operate on domain objects with compile lambda functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16bf256f-421b-45a5-8d54-2b5966ca81e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n|movieId|               title|              genres|\n+-------+--------------------+--------------------+\n|      1|    Toy Story (1995)|Adventure|Animati...|\n|      2|      Jumanji (1995)|Adventure|Childre...|\n|      3|Grumpier Old Men ...|      Comedy|Romance|\n|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n|      5|Father of the Bri...|              Comedy|\n|      6|         Heat (1995)|Action|Crime|Thri...|\n|      7|      Sabrina (1995)|      Comedy|Romance|\n|      8| Tom and Huck (1995)|  Adventure|Children|\n|      9| Sudden Death (1995)|              Action|\n|     10|    GoldenEye (1995)|Action|Adventure|...|\n|     11|American Presiden...|Comedy|Drama|Romance|\n|     12|Dracula: Dead and...|       Comedy|Horror|\n|     13|        Balto (1995)|Adventure|Animati...|\n|     14|        Nixon (1995)|               Drama|\n|     15|Cutthroat Island ...|Action|Adventure|...|\n|     16|       Casino (1995)|         Crime|Drama|\n|     17|Sense and Sensibi...|       Drama|Romance|\n|     18|   Four Rooms (1995)|              Comedy|\n|     19|Ace Ventura: When...|              Comedy|\n|     20|  Money Train (1995)|Action|Comedy|Cri...|\n+-------+--------------------+--------------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "sparkdf=spark.read.csv(\"dbfs:/FileStore/tables/movies.csv\",header='True')\n",
    "sparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "082a6db0-a6bf-4799-86b7-79b5c449e190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---+\n|Seqno|               Quote| id|\n+-----+--------------------+---+\n|    1|Be the change tha...|  1|\n|    2|Everyone thinks o...|  2|\n|    3|The purpose of ou...|  3|\n|    4|            Be cool.|  4|\n+-----+--------------------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('pysparkseries').getOrCreate()\n",
    "columns = [\"Seqno\",\"Quote\",\"id\"]\n",
    "data = [(\"1\", \"Be the change that you wish to see in the world\",1),\n",
    "    (\"2\", \"Everyone thinks of changing the world, but no one thinks of changing himself.\",2),\n",
    "    (\"3\", \"The purpose of our lives is to be happy.\",3),\n",
    "    (\"4\", \"Be cool.\",4)]\n",
    "df1 = spark.createDataFrame(data,columns)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6ae4062-d1f5-4344-9cb6-3945c7afe37c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------+---+\n|Seqno|Quote                                                                        |id |\n+-----+-----------------------------------------------------------------------------+---+\n|1    |Be the change that you wish to see in the world                              |1  |\n|2    |Everyone thinks of changing the world, but no one thinks of changing himself.|2  |\n|3    |The purpose of our lives is to be happy.                                     |3  |\n|4    |Be cool.                                                                     |4  |\n+-----+-----------------------------------------------------------------------------+---+\n\n"
     ]
    }
   ],
   "source": [
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c0f8522-6af8-4071-95f2-8e1fe383d41b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------+---+\n|Seqno|Quote                                                                        |id |\n+-----+-----------------------------------------------------------------------------+---+\n|1    |Be the change that you wish to see in the world                              |1  |\n|2    |Everyone thinks of changing the world, but no one thinks of changing himself.|2  |\n+-----+-----------------------------------------------------------------------------+---+\nonly showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "df1.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33bd4bef-b525-4f4c-966a-7c3bb9600cb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#PySpark StructType & StructField Explained with Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5314ab0-9937-4d8c-811e-06c19ce84978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#PySpark infers a schema from data, sometimes we may need to define our own column names and data types \n",
    "#PySpark provides from pyspark.sql.types import StructType class to define the structure of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8083705a-29c0-4745-9efa-7f4aa98767a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---------+----------+--------+-----+------+------+\n|firstname|middlename|lastname|id   |gender|salary|\n+---------+----------+--------+-----+------+------+\n|James    |          |Smith   |36636|M     |3000  |\n|Michael  |Rose      |        |40288|M     |4000  |\n|Robert   |          |Williams|42114|M     |4000  |\n|Maria    |Anne      |Jones   |39192|F     |4000  |\n|Jen      |Mary      |Brown   |     |F     |-1    |\n+---------+----------+--------+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName('pysparkseries').getOrCreate()\n",
    "\n",
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \n",
    "    StructField(\"firstname\",StringType(),True), \n",
    "    StructField(\"middlename\",StringType(),True), \n",
    "    StructField(\"lastname\",StringType(),True), \n",
    "    StructField(\"id\", StringType(), True), \n",
    "    StructField(\"gender\", StringType(), True), \n",
    "    StructField(\"salary\", IntegerType(), True) \n",
    "  ])\n",
    " \n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce583d6-acb9-4547-b267-ad18831ef6db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n |-- _3: string (nullable = true)\n |-- _4: string (nullable = true)\n |-- _5: string (nullable = true)\n |-- _6: long (nullable = true)\n\n+-------+----+--------+-----+---+----+\n|_1     |_2  |_3      |_4   |_5 |_6  |\n+-------+----+--------+-----+---+----+\n|James  |    |Smith   |36636|M  |3000|\n|Michael|Rose|        |40288|M  |4000|\n|Robert |    |Williams|42114|M  |4000|\n|Maria  |Anne|Jones   |39192|F  |4000|\n|Jen    |Mary|Brown   |     |F  |-1  |\n+-------+----+--------+-----+---+----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName('pysparkseries').getOrCreate()\n",
    "\n",
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "\n",
    " \n",
    "df = spark.createDataFrame(data=data)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d81dc337-020c-4d98-9ed2-beef500c7a97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#PySpark Select Columns From DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f20b9e0-a07c-4dc7-940f-e8b5f581c31a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n|firstname|lastname|\n+---------+--------+\n|    James|   Smith|\n|  Michael|        |\n|   Robert|Williams|\n|    Maria|   Jones|\n|      Jen|   Brown|\n+---------+--------+\n\n+---------+--------+\n|firstname|lastname|\n+---------+--------+\n|    James|   Smith|\n|  Michael|        |\n|   Robert|Williams|\n|    Maria|   Jones|\n|      Jen|   Brown|\n+---------+--------+\n\n+---------+--------+\n|firstname|lastname|\n+---------+--------+\n|    James|   Smith|\n|  Michael|        |\n|   Robert|Williams|\n|    Maria|   Jones|\n|      Jen|   Brown|\n+---------+--------+\n\n+---------+--------+\n|firstname|lastname|\n+---------+--------+\n|    James|   Smith|\n|  Michael|        |\n|   Robert|Williams|\n|    Maria|   Jones|\n|      Jen|   Brown|\n+---------+--------+\n\n+---------+----------+--------+\n|firstname|middlename|lastname|\n+---------+----------+--------+\n|    James|          |   Smith|\n|  Michael|      Rose|        |\n|   Robert|          |Williams|\n|    Maria|      Anne|   Jones|\n|      Jen|      Mary|   Brown|\n+---------+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.select(\"firstname\",\"lastname\").show()\n",
    "\n",
    "df.select(df.firstname,df.lastname).show()\n",
    "\n",
    "df.select(df[\"firstname\"],df[\"lastname\"]).show()\n",
    "\n",
    "#By using col() function\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"firstname\"),col(\"lastname\")).show()\n",
    "\n",
    "#Select columns by regular expression\n",
    "df.select(df.colRegex(\"`^.*name*`\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed7a2511-1818-4506-90b7-5b4c91d9beb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n|firstname|middlename|lastname|   id|gender|salary|\n+---------+----------+--------+-----+------+------+\n|    James|          |   Smith|36636|     M|  3000|\n|  Michael|      Rose|        |40288|     M|  4000|\n|   Robert|          |Williams|42114|     M|  4000|\n|    Maria|      Anne|   Jones|39192|     F|  4000|\n|      Jen|      Mary|   Brown|     |     F|    -1|\n+---------+----------+--------+-----+------+------+\n\n+---------+----------+--------+-----+------+------+\n|firstname|middlename|lastname|   id|gender|salary|\n+---------+----------+--------+-----+------+------+\n|    James|          |   Smith|36636|     M|  3000|\n|  Michael|      Rose|        |40288|     M|  4000|\n|   Robert|          |Williams|42114|     M|  4000|\n|    Maria|      Anne|   Jones|39192|     F|  4000|\n|      Jen|      Mary|   Brown|     |     F|    -1|\n+---------+----------+--------+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select All columns\n",
    "df.select([col for col in df.columns]).show()\n",
    "df.select(\"*\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69314c0f-f528-4041-8532-f4540e969684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Select Columns by Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e4068e3-537c-4d33-ab21-d56262b261fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+\n|firstname|middlename|lastname|\n+---------+----------+--------+\n|    James|          |   Smith|\n|  Michael|      Rose|        |\n|   Robert|          |Williams|\n|    Maria|      Anne|   Jones|\n+---------+----------+--------+\nonly showing top 4 rows\n\n+--------+-----+\n|lastname|   id|\n+--------+-----+\n|   Smith|36636|\n|        |40288|\n|Williams|42114|\n|   Jones|39192|\n+--------+-----+\nonly showing top 4 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Selects first 3 columns and top 4 rows\n",
    "df.select(df.columns[:3]).show(4)\n",
    "\n",
    "#Selects columns 2 to 4  and top 4 rows\n",
    "df.select(df.columns[2:4]).show(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2c949a5-12d6-420c-8eb4-203665507b61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- state: string (nullable = true)\n |-- gender: string (nullable = true)\n\n+----------------------+-----+------+\n|name                  |state|gender|\n+----------------------+-----+------+\n|{James, null, Smith}  |OH   |M     |\n|{Anna, Rose, }        |NY   |F     |\n|{Julia, , Williams}   |OH   |F     |\n|{Maria, Anne, Jones}  |NY   |M     |\n|{Jen, Mary, Brown}    |NY   |M     |\n|{Mike, Mary, Williams}|OH   |M     |\n+----------------------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "#4. Select Nested Struct Columns from PySpark\n",
    "\n",
    "data = [\n",
    "        ((\"James\",None,\"Smith\"),\"OH\",\"M\"),\n",
    "        ((\"Anna\",\"Rose\",\"\"),\"NY\",\"F\"),\n",
    "        ((\"Julia\",\"\",\"Williams\"),\"OH\",\"F\"),\n",
    "        ((\"Maria\",\"Anne\",\"Jones\"),\"NY\",\"M\"),\n",
    "        ((\"Jen\",\"Mary\",\"Brown\"),\"NY\",\"M\"),\n",
    "        ((\"Mike\",\"Mary\",\"Williams\"),\"OH\",\"M\")\n",
    "        ]\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType        \n",
    "schema = StructType([\n",
    "    StructField('name', StructType([\n",
    "         StructField('firstname', StringType(), True),\n",
    "         StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True)\n",
    "         ])),\n",
    "     StructField('state', StringType(), True),\n",
    "     StructField('gender', StringType(), True)\n",
    "     ])\n",
    "df2 = spark.createDataFrame(data = data, schema = schema)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False) # shows all columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "426b47a9-9f68-4012-9d60-3a16be105fad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n|name                  |\n+----------------------+\n|{James, null, Smith}  |\n|{Anna, Rose, }        |\n|{Julia, , Williams}   |\n|{Maria, Anne, Jones}  |\n|{Jen, Mary, Brown}    |\n|{Mike, Mary, Williams}|\n+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df2.select(\"name\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12be48f6-d817-4c86-b6b5-18c68099af6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----+------+\n|name                  |state|gender|\n+----------------------+-----+------+\n|{James, null, Smith}  |OH   |M     |\n|{Anna, Rose, }        |NY   |F     |\n|{Julia, , Williams}   |OH   |F     |\n|{Maria, Anne, Jones}  |NY   |M     |\n|{Jen, Mary, Brown}    |NY   |M     |\n|{Mike, Mary, Williams}|OH   |M     |\n+----------------------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df2.select(\"*\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7209bce3-9a7d-4c6b-9392-65612e42a99b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n|firstname|lastname|\n+---------+--------+\n|James    |Smith   |\n|Anna     |        |\n|Julia    |Williams|\n|Maria    |Jones   |\n|Jen      |Brown   |\n|Mike     |Williams|\n+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df2.select(\"name.firstname\",\"name.lastname\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e784a2e-b54d-4702-918e-3bc6efbf38e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#PySpark Collect() – Retrieve data from DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec7fb2df-db49-490d-a52d-dffd44f0e201",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PySpark RDD/DataFrame collect() is an action operation that is used to retrieve all the elements of the dataset (from all nodes) to the driver node. We should use the collect() on smaller dataset usually after filter(), group() e.t.c. Retrieving larger datasets results in OutOfMemory error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a1444e2-c9c2-4ca6-b135-bac4f40840ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('pysparkseries').getOrCreate()\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.show(truncate=False)\n",
    "\n",
    "dataCollect = deptDF.collect()\n",
    "print(dataCollect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da71d0e2-b490-4977-b32a-9abcfb56c31d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[28]: Row(dept_name='Finance', dept_id=10)"
     ]
    }
   ],
   "source": [
    "deptDF.collect() #returns Array of Row type.\n",
    "deptDF.collect()[0] #returns the first element in an array (1st row).\n",
    "#deptDF.collect[0][0] #returns the value of the first row & first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8c2abdb-ae83-4ebc-9ddc-3e63631c2e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "When to avoid Collect()\n",
    "Usually, collect() is used to retrieve the action output when you have very small result set and calling collect() on an RDD/DataFrame with a bigger result set causes out of memory as it returns the entire dataset (from all workers) to the driver hence we should avoid calling collect() on a larger dataset.\n",
    "\n",
    "collect () vs select ()\n",
    "collect is an action\n",
    "select() is a transformation that returns a new DataFrame and holds the columns that are selected whereas collect() is an action that returns the entire data set in an Array to the driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b352acc-54a9-4dba-9e0b-79d71baec5e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#PySpark withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4629237-d4c8-414d-9d10-35f944339fcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('pysparkseries').getOrCreate()\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b222100e-6343-4e2e-9c57-6300653c8217",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n|firstname|middlename|lastname|       dob|gender|salary|\n+---------+----------+--------+----------+------+------+\n|    James|          |   Smith|1991-04-01|     M|  3000|\n|  Michael|      Rose|        |2000-05-19|     M|  4000|\n|   Robert|          |Williams|1978-09-05|     M|  4000|\n|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n+---------+----------+--------+----------+------+------+\n\nroot\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df.withColumn(\"salary\",col(\"salary\").cast(\"string\")).show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67054fae-cdab-4d16-b043-7c424ece82ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n|firstname|middlename|lastname|       dob|gender|salary|\n+---------+----------+--------+----------+------+------+\n|    James|          |   Smith|1991-04-01|     M|  3000|\n|  Michael|      Rose|        |2000-05-19|     M|  4000|\n|   Robert|          |Williams|1978-09-05|     M|  4000|\n|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n+---------+----------+--------+----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"salary\",col(\"salary\").cast(\"string\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04ba73b4-328c-436c-bd88-7d766fc93259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77b4b6d9-865d-457f-9f5e-05eadcd6caff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+-------+\n|firstname|middlename|lastname|       dob|gender| salary|\n+---------+----------+--------+----------+------+-------+\n|    James|          |   Smith|1991-04-01|     M|3000000|\n|  Michael|      Rose|        |2000-05-19|     M|4000000|\n|   Robert|          |Williams|1978-09-05|     M|4000000|\n|    Maria|      Anne|   Jones|1967-12-01|     F|4000000|\n|      Jen|      Mary|   Brown|1980-02-17|     F|  -1000|\n+---------+----------+--------+----------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.withColumn(\"salary\",col(\"salary\")*1000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "788310cd-377a-45c9-bc61-03ee3e92b5b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#PySpark withColumnRenamed to Rename Column on DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfa383c7-b770-40c3-aeae-e774e119704a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- DateOfBirth: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.withColumnRenamed(\"dob\",\"DateOfBirth\").printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8af3092-5fa3-4193-9b74-de67b0006507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Since DataFrame’s are an immutable collection, you can’t rename or update a column instead when using withColumnRenamed() it creates a new DataFrame with updated column names, In this PySpark article, I will cover different ways to rename columns with several use cases like rename nested column, all columns, selected multiple columns with Python/PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "564fd907-8f59-4bd6-bd70-066dbb05f3f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#PySpark Where Filter Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60467e77-2db5-4a5c-845a-22c2c681a28d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n|   Name|Age|\n+-------+---+\n|  Alice| 25|\n|    Bob| 30|\n|Charlie| 35|\n|  David| 40|\n|    Eve| 45|\n+-------+---+\n\n+-------+---+\n|   Name|Age|\n+-------+---+\n|Charlie| 35|\n|  David| 40|\n|    Eve| 45|\n+-------+---+\n\n+-------+---+\n|   Name|Age|\n+-------+---+\n|Charlie| 35|\n|  David| 40|\n|    Eve| 45|\n+-------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"FilterExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"Alice\", 25),\n",
    "        (\"Bob\", 30),\n",
    "        (\"Charlie\", 35),\n",
    "        (\"David\", 40),\n",
    "        (\"Eve\", 45)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "#Filter\n",
    "filtered_df = df.filter(df.Age > 30)\n",
    "filtered_df.show()\n",
    "\n",
    "#Where condition\n",
    "filtered_df1=df.where(df.Age > 30)\n",
    "filtered_df1.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "282ca9e0-66b7-4f56-9224-17cbd31bde45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#PySpark Distinct to Drop Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71aa5e5c-5817-469a-bc16-54dde1731eb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PySpark distinct() function is used to drop/remove the duplicate rows (all columns) from DataFrame and dropDuplicates() is used to drop rows based on selected (one or multiple) columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07cf8fa-3488-4ec7-bc41-b91bdcfecfc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import pySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName('sparkseries').getOrCreate()\n",
    "\n",
    "# Prepare Data\n",
    "data = [(\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600), \\\n",
    "    (\"Robert\", \"Sales\", 4100), \\\n",
    "    (\"Maria\", \"Finance\", 3000), \\\n",
    "    (\"James\", \"Sales\", 3000), \\\n",
    "    (\"Scott\", \"Finance\", 3300), \\\n",
    "    (\"Jen\", \"Finance\", 3900), \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000), \\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  ]\n",
    "\n",
    "# Create DataFrame\n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1d4e02c-843d-4c8e-b66d-cf78b35809a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct count: 9\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "distinctDF = df.distinct()\n",
    "print(\"Distinct count: \"+str(distinctDF.count()))\n",
    "distinctDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "491ccb02-dce5-408c-b0e4-7388b4f7d217",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct count: 9\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df2 = df.dropDuplicates()\n",
    "print(\"Distinct count: \"+str(df2.count()))\n",
    "df2.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a825980f-c3a3-4231-90a6-705f88efdf3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct count of department & salary : 8\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|Maria        |Finance   |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Kumar        |Marketing |2000  |\n|Jeff         |Marketing |3000  |\n|James        |Sales     |3000  |\n|Robert       |Sales     |4100  |\n|Michael      |Sales     |4600  |\n+-------------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#PySpark Distinct of Selected Multiple Columns\n",
    "dropDisDF = df.dropDuplicates([\"department\",\"salary\"])\n",
    "print(\"Distinct count of department & salary : \"+str(dropDisDF.count()))\n",
    "dropDisDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e41db12-0ffc-4d71-a50e-370286799985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#PySpark orderBy() and sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06244948-e127-4879-933d-54706d5752f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "You can use either sort() or orderBy() function of PySpark DataFrame to sort DataFrame by ascending or descending order based on single or multiple columns, you can also do sorting using PySpark SQL sorting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60aecc45-d538-4c9b-8218-b401fbabfb91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n+-------------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f4860de-b24e-446a-83dc-ddce397c17e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|James        |Sales     |NY   |90000 |34 |10000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|James        |Sales     |NY   |90000 |34 |10000|\n+-------------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.sort(\"department\",\"state\").show(truncate=False)\n",
    "df.sort(col(\"department\"),col(\"state\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24f3ac63-1c61-468b-b5f1-d4d7abeda5c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|James        |Sales     |NY   |90000 |34 |10000|\n+-------------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.orderBy(\"department\",\"state\").show(truncate=False)\n",
    "df.orderBy(col(\"department\"),col(\"state\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "823db9b6-dbcb-4345-bde6-b2dbd19b8fc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|James        |Sales     |NY   |90000 |34 |10000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|James        |Sales     |NY   |90000 |34 |10000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n+-------------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.sort(df.department.asc(),df.state.asc()).show(truncate=False)\n",
    "df.sort(col(\"department\").asc(),col(\"state\").asc()).show(truncate=False)\n",
    "df.orderBy(col(\"department\").asc(),col(\"state\").asc()).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6ca9588-bdcc-4d9e-a10f-b1b750014488",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#PySpark Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be68f615-011c-4f16-96ce-2619c6d88817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PySpark groupBy() function is used to collect the identical data into groups on DataFrame and perform count, sum, avg, min, max functions on the grouped data. In this article, I will explain several groupBy() examples using PySpark (Spark with Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a607cad2-66ab-49d2-9217-03239dd90cfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n+-------------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "697fcc30-6ec4-4244-ab21-dc21a8d20595",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n|department|sum(salary)|\n+----------+-----------+\n|Sales     |257000     |\n|Finance   |351000     |\n|Marketing |171000     |\n+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.groupBy(\"department\").sum(\"salary\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a6c484d-2615-4e6d-8cb1-3536a7918e10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[44]: DataFrame[department: string, min(salary): bigint]"
     ]
    }
   ],
   "source": [
    "\n",
    "df.groupBy(\"department\").min(\"salary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2da526b0-2a0a-4ef5-baaa-d92ad03894dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+---------+\n|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n+----------+----------+-----------------+---------+---------+\n|Sales     |257000    |85666.66666666667|53000    |23000    |\n|Finance   |351000    |87750.0          |81000    |24000    |\n+----------+----------+-----------------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import sum,avg,max\n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "      avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "      sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "      max(\"bonus\").alias(\"max_bonus\")) \\\n",
    "    .where(col(\"sum_bonus\") >= 50000) \\\n",
    "    .show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3bb0f44-11f6-4115-bd00-192663449d2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#PySpark Join Types | Join Two DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8c83b8e-8797-4df9-ad77-97d36fc041cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PySpark Join is used to combine two DataFrames and by chaining these you can join multiple DataFrames; it supports all basic join type operations available in traditional SQL like INNER, LEFT OUTER, RIGHT OUTER, LEFT ANTI, LEFT SEMI, CROSS, SELF JOIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a506a3d9-6f88-4ce7-81a0-929a691c85d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- emp_id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- superior_emp_id: long (nullable = true)\n |-- year_joined: string (nullable = true)\n |-- emp_dept_id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n|6     |Brown   |2              |2010       |50         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7f234d6-5d22-4fa3-ad3e-0177c764b5c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# PySpark Inner Join DataFrame\n",
    "\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n",
    "     .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adf78f7a-dbe3-4336-9a25-cada269315d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# PySpark Inner Join DataFrame\n",
    "\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\") \\\n",
    "     .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c774458e-d375-4068-93a9-70ca1dc9db68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# PySpark Inner Join DataFrame\n",
    "\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\") \\\n",
    "     .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd9f5532-1681-4bce-be1d-2b32a7a88dfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#PySpark Full Outer Join\n",
    "\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\") \\\n",
    "    .show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\") \\\n",
    "    .show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\") \\\n",
    "    .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8106a4df-80c5-400d-8632-e013c32447eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "#PySpark Left Outer Join\n",
    "\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\")\\\n",
    "    .show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\")\\\n",
    "    .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c4b23b8-c00a-4c9b-8589-ffae7297f22f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\") \\\n",
    "   .show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"rightouter\") \\\n",
    "   .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a5b2fa7-d3ba-48a9-9b96-8f53071775d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- emp_id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- superior_emp_id: long (nullable = true)\n |-- year_joined: string (nullable = true)\n |-- emp_dept_id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n|6     |Brown   |2              |2010       |50         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\n+------+-----+---------------+-----------+-----------+------+------+\n|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+-----+---------------+-----------+-----------+------+------+\n|6     |Brown|2              |2010       |50         |      |-1    |\n+------+-----+---------------+-----------+-----------+------+------+\n\n+------+--------+---------------+-----------------+\n|emp_id|name    |superior_emp_id|superior_emp_name|\n+------+--------+---------------+-----------------+\n|2     |Rose    |1              |Smith            |\n|3     |Williams|1              |Smith            |\n|4     |Jones   |2              |Rose             |\n|5     |Brown   |2              |Rose             |\n|6     |Brown   |2              |Rose             |\n+------+--------+---------------+-----------------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName('pysparkseries').getOrCreate()\n",
    "\n",
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)\n",
    "\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)\n",
    "  \n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n",
    "     .show(truncate=False)\n",
    "\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\") \\\n",
    "    .show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\") \\\n",
    "    .show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\") \\\n",
    "    .show(truncate=False)\n",
    "    \n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\") \\\n",
    "    .show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\") \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\") \\\n",
    "   .show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"rightouter\") \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftsemi\") \\\n",
    "   .show(truncate=False)\n",
    "   \n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\") \\\n",
    "   .show(truncate=False)\n",
    "   \n",
    "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"), \\\n",
    "    col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\"inner\") \\\n",
    "    .select(col(\"emp1.emp_id\"),col(\"emp1.name\"), \\\n",
    "      col(\"emp2.emp_id\").alias(\"superior_emp_id\"), \\\n",
    "      col(\"emp2.name\").alias(\"superior_emp_name\")) \\\n",
    "   .show(truncate=False)\n",
    "\n",
    "empDF.createOrReplaceTempView(\"EMP\")\n",
    "deptDF.createOrReplaceTempView(\"DEPT\")\n",
    "   \n",
    "joinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\") \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "joinDF2 = spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n",
    "  .show(truncate=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day 2 PYSPARK COMPLETE FUNCTIONS",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}